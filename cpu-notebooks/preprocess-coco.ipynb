{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../')\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "survey_path='/nfs/juhu/data/rakhasan/bystander-detection/pilot3_coco/'\n",
    "survey_photo_path = survey_path+'/photos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = pickle.load(open(survey_path +'mappings_common.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agree_disagree = {'Strongly disagree' : -3, 'Disagree': -2, 'Somewhat disagree' : -1, 'Neither agree nor disagree' : 0,\n",
    "                  'Somewhat agree': 1, 'Agree':2, 'Strongly agree' : 3}\n",
    "\n",
    "photo_place = {'A public place':-2, 'A semi-public place.':-1, 'A semi-private place':1, 'A private place':2, 'Not sure':0}\n",
    "\n",
    "awareness= {'Not at all aware':0, 'Slightly aware':1, 'Somewhat aware':2,'Moderately aware':3, 'Extremely aware':4}\n",
    "\n",
    "willingness = {'Completely unwilling':-2,'Somewhat unwilling':-1, 'Neither unwilling nor willing':0,\n",
    "               'Somewhat willing':1, 'Completely willing':2}\n",
    "\n",
    "comfortable = {'Highly uncomfortable':-3, 'Uncomfortable':-2, 'Somewhat uncomfortable':-1, 'Neither uncomfortable nor comfortable':0,\n",
    "              'Somewhat comfortable':1, 'Comfortable':2, 'Highly comfortable':3}\n",
    "\n",
    "subject = {'Definitely subject':2, 'Most probably subject':1, 'Not sure':0, \n",
    "           'Most probably bystander':-1, 'Definitely bystander':-2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_data = ['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "              'Finished',\n",
    "             'RecordedDate', 'ResponseId', 'RecipientLastName',\n",
    "             'RecipientFirstName', 'RecipientEmail', 'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "             'DistributionChannel', 'UserLanguage', \n",
    "             'count', \n",
    "             #'path', 'num_of_check', 'img_w', 'img_h', 'total_count', 'img_w2', 'img_h2', \n",
    "             'RandomCode',\n",
    "             'IsMobile', 'test',\n",
    "            ]\n",
    "test_questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_invalid_responses(df,                             \n",
    "                             tolerance = 0):\n",
    "    '''Remove unfinished or responses with incorrect attention check > tolerance'''\n",
    "    rows_to_remove = []\n",
    "    not_finished = 0\n",
    "    no_id=0\n",
    "    duplicate=0\n",
    "    questions_missed=0\n",
    "    no_sequence=0\n",
    "    for index, row in df.iterrows():\n",
    "        if int(row['duplicate'])==1:\n",
    "            rows_to_remove.append(index)\n",
    "            duplicate+=1\n",
    "            continue\n",
    "            \n",
    "        if row['Finished'] =='False':\n",
    "            rows_to_remove.append(index)\n",
    "            not_finished +=1\n",
    "            continue\n",
    "        workerid = row['workerid']\n",
    "        if type(workerid)==float or len(workerid.strip())==0:\n",
    "            rows_to_remove.append(index)\n",
    "            no_id +=1\n",
    "            continue\n",
    "        if row['sequence']=='s':\n",
    "            no_sequence+=1\n",
    "            rows_to_remove.append(index)\n",
    "            continue\n",
    "        if (isinstance(row['attnwrong'], float) and np.isnan(row['attnwrong'])) or \\\n",
    "            int(row['attnwrong']) > tolerance:\n",
    "                rows_to_remove.append(index)\n",
    "                questions_missed+=1\n",
    "    print('duplicate:{}, unfinished:{}, no workerid: {}, no sequence: {}, questions missed:{}'.format(\n",
    "        duplicate, not_finished, no_id, no_sequence, questions_missed))    \n",
    "    #print('rows removed:',rows_to_remove)\n",
    "    removed_rows =  df.loc[rows_to_remove]\n",
    "    df= df.drop(rows_to_remove, axis=0, inplace=False)\n",
    "    return (df, removed_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":{\"progressId\":\"ES_cvX6DEnkVLZjDvL\",\"percentComplete\":0.0,\"status\":\"inProgress\"},\"meta\":{\"requestId\":\"4b8cff65-41b6-4242-b57b-6b9557d2156d\",\"httpStatus\":\"200 - OK\"}}\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 100.0 complete\n",
      "Complete\n",
      "{\"result\":{\"progressId\":\"ES_bllADrFbR0KbJkh\",\"percentComplete\":0.0,\"status\":\"inProgress\"},\"meta\":{\"requestId\":\"c6e82575-3f94-4aa0-8384-d2d512f27d64\",\"httpStatus\":\"200 - OK\"}}\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 100.0 complete\n",
      "Complete\n",
      "{\"result\":{\"progressId\":\"ES_aXWVUqW6c1yiYiF\",\"percentComplete\":0.0,\"status\":\"inProgress\"},\"meta\":{\"requestId\":\"7f572c6b-9eea-462c-b25f-92a3d3eba09b\",\"httpStatus\":\"200 - OK\"}}\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 100.0 complete\n",
      "Complete\n",
      "{\"result\":{\"progressId\":\"ES_07j1Xmt8edr255r\",\"percentComplete\":0.0,\"status\":\"inProgress\"},\"meta\":{\"requestId\":\"d496f425-62dc-4917-9e14-db0971a57558\",\"httpStatus\":\"200 - OK\"}}\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 100.0 complete\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python  /data/rakhasan/download_qualtrics_data.py SV_6Ya2ALuITeOYWvX /data/rakhasan/bystander-detection/pilot3_coco/survey-response.csv\n",
    "python  /data/rakhasan/download_qualtrics_data.py SV_e5O28IS76M968GV /data/rakhasan/bystander-detection/pilot3_coco/survey-response2.csv \n",
    "python  /data/rakhasan/download_qualtrics_data.py SV_3f4VyiHf6GlLJrf /data/rakhasan/bystander-detection/pilot3_coco/fourth-response.csv \n",
    "python  /data/rakhasan/download_qualtrics_data.py SV_8qRBx0kNIaSmc3b /data/rakhasan/bystander-detection/pilot3_coco/fourth-response2.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of responses:  104\n",
      "duplicate:27, unfinished:0, no workerid: 2, no sequence: 7, questions missed:11\n",
      "57 47\n",
      "duplicate:27, unfinished:0, no workerid: 2, no sequence: 7, questions missed:1\n",
      "67 37\n",
      "duplicate:27, unfinished:0, no workerid: 2, no sequence: 7, questions missed:0\n",
      "68 36\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(survey_path,'survey-response.csv')\n",
    "data_file2 = os.path.join(survey_path,'survey-response2.csv')\n",
    "\n",
    "data1=pd.read_csv(data_file).reset_index(inplace=False)\n",
    "data1.drop([0,1], inplace=True, axis=0) #drop first row\n",
    "\n",
    "data2=pd.read_csv(data_file2).reset_index(inplace=False)\n",
    "data2.drop([0,1], inplace=True, axis=0) #drop first row\n",
    "\n",
    "## A hack to keep one data point\n",
    "i=data2[data2.photo_set=='14'].workerid.index\n",
    "data2.loc[i,'workerid']='temp'\n",
    "\n",
    "data = pd.concat([data1,data2])\n",
    "data.index = range(len(data))\n",
    "\n",
    "print('Total number of responses: ',len(data))\n",
    "for t in range(3):\n",
    "    df, removed = remove_invalid_responses(data, tolerance=t) \n",
    "    print(len(df),len(removed))\n",
    "#     print('Tolerance: {}, valid response: {}({:.2f}%) (not finished: {})'.format(\n",
    "#         t, len(validated_data),len(validated_data)*100/(len(data)-not_finished), not_finished))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of responses:  4\n",
      "duplicate:0, unfinished:0, no workerid: 0, no sequence: 0, questions missed:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1832)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fourth response\n",
    "data_file = os.path.join(survey_path,'fourth-response.csv')\n",
    "data_file2 = os.path.join(survey_path,'fourth-response2.csv')\n",
    "\n",
    "data1=pd.read_csv(data_file).reset_index(inplace=False)\n",
    "data1.drop([0,1], inplace=True, axis=0) #drop first row\n",
    "\n",
    "data2=pd.read_csv(data_file2).reset_index(inplace=False)\n",
    "data2.drop([0,1], inplace=True, axis=0) #drop first row\n",
    "\n",
    "fourth = pd.concat([data1,data2])\n",
    "fourth.index = range(len(fourth))\n",
    "\n",
    "print('Total number of responses: ',len(fourth))\n",
    "fourth, removed = remove_invalid_responses(fourth, tolerance=0) \n",
    "fourth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate:27, unfinished:0, no workerid: 2, no sequence: 7, questions missed:11\n",
      "Final valid response with 0 tolerance: 57\n",
      "Number of photo sets:  18\n",
      "\n",
      "Removed mturk ids:\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workerid</th>\n",
       "      <th>attnwrong</th>\n",
       "      <th>photo_set</th>\n",
       "      <th>duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2615YW1YERQBO</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A2MCNGY62MPRI5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>A19PN52BDA462L</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A2U0JT7TSIIXPS</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AG9LWKO86TNHG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A2VKACLXTMOQWO</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>A2K4OJDQPXIU5T</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>A28GFEMPMLUU14</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>A2O2Y99RA9GFUJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>A2RY24DUOWXUWU</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>A26MN6JIKD4NXU</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>A3W5BTNHDA9U64</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>A382SL9ROIY1P6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>adf</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>A3HZFB2JLF3JMY</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>ARGR4AJESGHHP</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>temp</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           workerid attnwrong photo_set duplicate\n",
       "18   A2615YW1YERQBO         1         8         0\n",
       "23   A2MCNGY62MPRI5         1        11         0\n",
       "45   A19PN52BDA462L         1         2         0\n",
       "46   A2U0JT7TSIIXPS         1         9         0\n",
       "48    AG9LWKO86TNHG         1         0         0\n",
       "49   A2VKACLXTMOQWO         1         9         0\n",
       "50   A2K4OJDQPXIU5T         0         4         0\n",
       "51   A28GFEMPMLUU14         0         9         0\n",
       "54   A2O2Y99RA9GFUJ         1         0         0\n",
       "57   A2RY24DUOWXUWU         0         2         0\n",
       "62              NaN         0        13         0\n",
       "68   A26MN6JIKD4NXU         0        16         0\n",
       "76   A3W5BTNHDA9U64         0        15         0\n",
       "80   A382SL9ROIY1P6         1        15         0\n",
       "81              adf         2        15         0\n",
       "87              NaN         0         6         0\n",
       "88   A3HZFB2JLF3JMY         1         9         0\n",
       "89    ARGR4AJESGHHP         1         6         0\n",
       "97             temp         0        14         0\n",
       "102              22         0        16         0"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=0\n",
    "validated_data, removed = remove_invalid_responses(data, tolerance=t) \n",
    "validated_data.index.rename('pid', inplace = True)\n",
    "#validated_data.photo_set_offset.fillna(0, inplace=True)\n",
    "#validated_data['photo_set']=validated_data.photo_set.astype(int)#.apply(lambda row: int(row.photo_set), axis=1)\n",
    "print('Final valid response with {} tolerance: {}'.format(t, len(validated_data)))\n",
    "print('Number of photo sets: ', len(validated_data.photo_set.unique()))\n",
    "photo_set_sizes = validated_data.groupby('photo_set').size()\n",
    "\n",
    "print('\\nRemoved mturk ids:')\n",
    "print(len(removed[(removed.duplicate=='0')][['workerid','attnwrong']]))\n",
    "removed[(removed.duplicate=='0')][['workerid','attnwrong','photo_set','duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "photo_set\n",
       "0     3\n",
       "1     3\n",
       "10    3\n",
       "11    3\n",
       "12    4\n",
       "13    3\n",
       "14    3\n",
       "15    3\n",
       "16    3\n",
       "17    3\n",
       "2     3\n",
       "3     3\n",
       "4     3\n",
       "5     3\n",
       "6     3\n",
       "7     5\n",
       "8     3\n",
       "9     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total:',len(set(validated_data['photo_set'])))\n",
    "validated_data.groupby('photo_set').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     57.000000\n",
       "mean     105.947368\n",
       "std       93.793546\n",
       "min        3.000000\n",
       "25%       56.000000\n",
       "50%       76.000000\n",
       "75%      127.000000\n",
       "max      548.000000\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_data['duration'] = validated_data['Duration (in seconds)'].astype(float)//60\n",
    "validated_data.duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "photo_ques_headers = ['contains_human', 'photo_place', 'was_aware', 'posing','comfort', \n",
    "                      'will', 'photographer_intention', 'replacable', 'subject_bystander', 'why_subject', \n",
    "                      'why_subject_text','why_bystander', 'why_bystander_text', 'why_neither', 'why_neither_text']\n",
    "\n",
    "high_level_concepts = ['was_aware', 'posing','comfort', 'will', 'photographer_intention', 'replacable', 'photo_place']\n",
    "high_level_concepts_num = [c+'_num' for c in high_level_concepts]\n",
    "\n",
    "questions = list(validated_data.columns)\n",
    "photo_questions = questions[questions.index('1_Q6.1'):questions.index('Q7.1')]\n",
    "photos_question_start = questions.index('1_Q6.1')\n",
    "questions_per_photo = 34 #including text responeses for last 3 questions\n",
    "\n",
    "def create_photo_df(df):\n",
    "    \n",
    "    dicts = []\n",
    "    for pid, row in df.iterrows():\n",
    "        per_participant_img_num =int( row['per_participant_img_num'])\n",
    "#         photo_set = int(row['photo_set'])    \n",
    "        sequence = list(filter(None, row['sequence'].split(',')))\n",
    "        if len(sequence)!=52:\n",
    "            print('pid: {}, len(seq): {}'.format(pid, len(sequence)))\n",
    "            continue\n",
    "        for p in range(len(sequence)): #for each photo\n",
    "            if sequence[p] == 'test1' or sequence[p] == 'test2':\n",
    "                continue\n",
    "            d = dict()\n",
    "            d['photo_no'] = int(sequence[p])\n",
    "            d['pid'] = row.name#['pid']\n",
    "            for q in range(10):               \n",
    "                d[photo_ques_headers[q]] = row[photo_questions[p * questions_per_photo + q]]    \n",
    "            dicts.append(d)\n",
    "    return pd.DataFrame.from_dict(dicts)\n",
    "                \n",
    "        \n",
    "def text_to_numeric_series(df, col_name, conversion_dict):\n",
    "    return df.apply(lambda row: conversion_dict[row[col_name]] if isinstance(row[col_name], str) else row[col_name], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique photos: 900\n"
     ]
    }
   ],
   "source": [
    "photo_df = create_photo_df(validated_data)\n",
    "photo_df.set_index('photo_no', inplace=True, drop=False)\n",
    "print('Total unique photos: {}'.format(len(set(photo_df.index.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fourth response'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fourth response'''\n",
    "# fourth = create_photo_df(fourth)\n",
    "# fourth.set_index('photo_no', inplace=True, drop=False)\n",
    "# print('Total unique photos: {}'.format(len(set(fourth.index.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 photos removed when 1 participants found no person in the photo (32.2%)\n",
      "116 photos removed when 2 participants found no person in the photo (12.9%)\n",
      "54 photos removed when 3 participants found no person in the photo (6.0%)\n",
      "Total unique photos containing people: 784, rows: 2480, removed:116\n"
     ]
    }
   ],
   "source": [
    "def remove_responses_non_person(df, tolerance= 0):\n",
    "    rows_to_remove = defaultdict(int)\n",
    "    for photo_no, row in df.iterrows():\n",
    "        if row['contains_human'].strip() =='I don\\'t see any box.'\\\n",
    "        or row['contains_human'].strip() =='There is just a depiction/representation of a person but not a real person (e.g. a poster/photo/sculpture of a person).'\\\n",
    "        or row['contains_human'].strip()=='I don\\'t see any box.'\\\n",
    "        or row['contains_human'].strip()=='There is something else inside the box.':\n",
    "            rows_to_remove[photo_no] += 1\n",
    "    to_remove = [k for k in rows_to_remove.keys() if rows_to_remove[k]>tolerance]\n",
    "    return to_remove, df.drop(to_remove, axis=0, inplace=False)\n",
    "\n",
    "for t in range(3):\n",
    "    removed_photos, valid_photo_df = remove_responses_non_person(photo_df, tolerance=t)\n",
    "    print('{} photos removed when {} participants found no person in the photo ({:.1f}%)'.format(\n",
    "        len(removed_photos), t+1, len(removed_photos)*100/len(set(photo_df.index.values))))\n",
    "\n",
    "removed_photos, valid_photo_df = remove_responses_non_person(photo_df, tolerance=1)\n",
    "print('Total unique photos containing people: {}, rows: {}, removed:{}'.format(\n",
    "    len(set(valid_photo_df.index.values)), len(valid_photo_df),len(removed_photos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''Copy removed images '''\n",
    "# import shutil\n",
    "# removed_photos, _ = remove_responses_non_person(photo_df, tolerance=0)\n",
    "# for r in removed_photos:\n",
    "#     shutil.copyfile(os.path.join(survey_path,'photos/',r+'.jpg'),os.path.join(survey_path,'removed-photos',r+'.jpg'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''Fourth'''\n",
    "# removed_photos, fourth = remove_responses_non_person(fourth, tolerance=0)\n",
    "# print('Total unique photos containing people: {}, rows: {}, removed:{}'.format(\n",
    "#     len(set(fourth.index.values)), len(fourth),len(removed_photos)))\n",
    "# fourth['subject_bystander_num'] = text_to_numeric_series(fourth, 'subject_bystander', subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def remove_responses_non_person(df, tolerance= 0):\n",
    "#     rows_to_remove = defaultdict(int)\n",
    "#     for photo_no, row in df.iterrows():\n",
    "#         if row['contains_human'].strip()!='There is a person with some of the major body parts visible (such as face, head, torso).':\n",
    "#             rows_to_remove[photo_no] += 1\n",
    "#     to_remove = [k for k in rows_to_remove.keys() if rows_to_remove[k]>tolerance]\n",
    "#     return to_remove, df.drop(to_remove, axis=0, inplace=False)\n",
    "\n",
    "# for t in range(3):\n",
    "#     removed_photos, valid_photo_df = remove_responses_non_person(photo_df, tolerance=t)\n",
    "#     print('{} photos removed when {} participants found no person in the photo ({:.1f}%)'.format(\n",
    "#         len(removed_photos), t+1, len(removed_photos)*100/len(set(photo_df.index.values))))\n",
    "\n",
    "# removed_photos, valid_photo_df = remove_responses_non_person(photo_df, tolerance=0)\n",
    "# print('Total unique photos containing people: {}, removed:{}'.format(\n",
    "#     len(set(valid_photo_df.index.values)),len(removed_photos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/l/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''Convert text responses to numbers for each photo'''\n",
    "\n",
    "valid_photo_df['subject_bystander_num'] = text_to_numeric_series(valid_photo_df, 'subject_bystander', subject)\n",
    "valid_photo_df['posing_num'] = text_to_numeric_series(valid_photo_df, 'posing', agree_disagree)\n",
    "valid_photo_df['photographer_intention_num'] = text_to_numeric_series(valid_photo_df, 'photographer_intention', agree_disagree)\n",
    "valid_photo_df['photo_place_num'] = text_to_numeric_series(valid_photo_df, 'photo_place', photo_place)\n",
    "valid_photo_df['will_num'] = text_to_numeric_series(valid_photo_df, 'will', willingness)\n",
    "valid_photo_df['comfort_num'] = text_to_numeric_series(valid_photo_df, 'comfort', comfortable)\n",
    "valid_photo_df['replacable_num'] = text_to_numeric_series(valid_photo_df, 'replacable', agree_disagree)\n",
    "valid_photo_df['was_aware_num'] = text_to_numeric_series(valid_photo_df, 'was_aware', agree_disagree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480 2306\n",
      "Total unique photos containing people: 784, rows:2306\n"
     ]
    }
   ],
   "source": [
    "'''Remove responses for photos where one or more people indicated that it does not contain any person'''\n",
    "print(len(valid_photo_df), len(valid_photo_df[~np.isnan(valid_photo_df.subject_bystander_num)]))\n",
    "valid_photo_df = valid_photo_df[~np.isnan(valid_photo_df.subject_bystander_num)]\n",
    "print('Total unique photos containing people: {}, rows:{}'.format(\n",
    "    len(set(valid_photo_df.index.values)), len(valid_photo_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_photo_df.subject_bystander_num.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 1974)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''check if any photo has less than two responses'''\n",
    "count=valid_photo_df.groupby('photo_no').subject_bystander_num.size()\n",
    "valid_photo_df = valid_photo_df.drop(count[count<3].index, inplace=False)\n",
    "valid_photo_df.to_pickle(os.path.join(survey_path,'3-response-df.pkl'))           \n",
    "valid_photo_df.subject_bystander_num.isnull().any(), len(valid_photo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(valid_photo_df.shape, fourth.shape)\n",
    "\n",
    "validated_data.to_pickle(os.path.join(survey_path, 'validated_df.pkl'))\n",
    "valid_photo_df.to_pickle(os.path.join(survey_path, 'photo_df.pkl'))\n",
    "#fourth.to_pickle(os.path.join(survey_path, 'fourth_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''Locations of the removed photos in the survey sequences'''\n",
    "# positions = [0]*52\n",
    "# position_dict = defaultdict(list)\n",
    "# for r in removed_photos:\n",
    "#     for s in list(validated_data.sequence):\n",
    "#         seq = s.split(',')\n",
    "#         if r in seq:\n",
    "#             position_dict[r].append(seq.index(r))\n",
    "#             positions[seq.index(r)]+=1\n",
    "\n",
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.bar(np.arange(len(positions)), positions)\n",
    "# plt.xticks(range(52),range(52), rotation=50)\n",
    "# plt.yticks(range(max(positions)+1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''A test to see if index in dataframe is same as the image ids'''\n",
    "# helper.draw_photos_from_path([survey_photo_path+'100.jpg', \n",
    "#                               helper.find_img_path(openImg_path, mapping[100][0]+'.jpg')], col_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Q8.1\n",
       " Female    17\n",
       " Male      26\n",
       " Name: ResponseId, dtype: int64, 57.4025974025974)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_data.groupby(['Q8.1']).ResponseId.count(),(221*100)/(221+164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Q8.2\n",
       " 4th-8th grade                   1\n",
       " Associate’s degree              9\n",
       " Bachelor's degree              18\n",
       " High school graduate or GED     7\n",
       " Master's Degree                 1\n",
       " Some college, no degree         7\n",
       " Name: ResponseId, dtype: int64, 4.0638297872340425, 1.5106382978723405)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_data.groupby(['Q8.2']).ResponseId.count(),191/len(validated_data), 71/len(validated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#validated_data.groupby(['Q8.3']).ResponseId.count(),242/len(validated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_data.groupby(['Q4.1']).ResponseId.count(),(154*100)/len(validated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_data.groupby(['Q7.1']).ResponseId.count(), 345/len(validated_data) #OSN account holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_data.groupby(['Q7.2']).ResponseId.count(), 30/len(validated_data) #OSN account holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
